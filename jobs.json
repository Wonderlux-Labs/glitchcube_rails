{
  "directory_name": "jobs",
  "files": [
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/ha_agent_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_summarizer_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/weather_forecast_summarizer_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/performance_mode_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/intermediate_summarizer_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_memory_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/gps_sensor_update_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/daily_summarizer_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/async_tool_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_timeout_monitor_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_memory_extraction_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/goal_monitor_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/application_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/host_registration_job.rb",
    "/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/world_state_updaters/narrative_conversation_sync_job.rb"
  ],
  "model_info": "ChatGPT models, text-embedding-ada-002",
  "prompt": "Project Path: jobs\n\nSource Tree:\n\n```\njobs\n‚îú‚îÄ‚îÄ ha_agent_job.rb\n‚îú‚îÄ‚îÄ conversation_summarizer_job.rb\n‚îú‚îÄ‚îÄ weather_forecast_summarizer_job.rb\n‚îú‚îÄ‚îÄ performance_mode_job.rb\n‚îú‚îÄ‚îÄ intermediate_summarizer_job.rb\n‚îú‚îÄ‚îÄ conversation_memory_job.rb\n‚îú‚îÄ‚îÄ gps_sensor_update_job.rb\n‚îú‚îÄ‚îÄ daily_summarizer_job.rb\n‚îú‚îÄ‚îÄ async_tool_job.rb\n‚îú‚îÄ‚îÄ conversation_timeout_monitor_job.rb\n‚îú‚îÄ‚îÄ conversation_memory_extraction_job.rb\n‚îú‚îÄ‚îÄ goal_monitor_job.rb\n‚îú‚îÄ‚îÄ application_job.rb\n‚îú‚îÄ‚îÄ host_registration_job.rb\n‚îî‚îÄ‚îÄ world_state_updaters\n    ‚îî‚îÄ‚îÄ narrative_conversation_sync_job.rb\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/ha_agent_job.rb`:\n\n```rb\n# app/jobs/ha_agent_job.rb\nclass HaAgentJob < ApplicationJob\n  queue_as :default\n\n  def perform(request:, tool_intents:, session_id:, conversation_id:, user_message:)\n    Rails.logger.info \"üè† HaAgentJob starting for session: #{session_id}\"\n    Rails.logger.info \"üìù Request: #{request}\"\n\n    begin\n      # Call Home Assistant's conversation.process API\n      response = call_ha_conversation_agent(request)\n\n      Rails.logger.info \"‚úÖ HA agent response received\"\n      Rails.logger.info \"üìÑ Response: #{response.inspect}\"\n\n      # Store results for next conversation turn (not as interrupting message)\n      store_ha_results(\n        session_id: session_id,\n        conversation_id: conversation_id,\n        user_message: user_message,\n        tool_intents: tool_intents,\n        ha_response: response\n      )\n\n      Rails.logger.info \"üíæ HA results stored for next conversation turn\"\n\n    rescue StandardError => e\n      Rails.logger.error \"‚ùå HaAgentJob failed: #{e.message}\"\n      Rails.logger.error e.backtrace.join(\"\\n\")\n\n      # Store failure for next turn\n      store_ha_results(\n        session_id: session_id,\n        conversation_id: conversation_id,\n        user_message: user_message,\n        tool_intents: tool_intents,\n        ha_response: nil,\n        error: e.message\n      )\n    end\n  end\n\n  private\n\n  def call_ha_conversation_agent(request)\n    Rails.logger.info \"üè† Calling HA conversation agent\"\n    Rails.logger.info \"üì§ Sending: #{request}\"\n\n    # Call actual Home Assistant conversation agent\n    HomeAssistantService.new.conversation_process(\n      text: request,\n      agent_id: \"conversation.claude_conversation\"\n    )\n  end\n\n  def store_ha_results(session_id:, conversation_id:, user_message:, tool_intents:, ha_response:, error: nil)\n    # Store results in conversation metadata, not as conversation log entries\n    # This avoids interrupting ongoing TTS/conversation\n\n    conversation = Conversation.find_by(id: conversation_id)\n    return unless conversation\n\n    # Get existing pending results or create new array\n    existing_metadata = conversation.metadata_json || {}\n    pending_results = existing_metadata[\"pending_ha_results\"] || []\n\n    # Create result entry\n    result_entry = {\n      timestamp: Time.current.iso8601,\n      user_message: user_message,\n      tool_intents: tool_intents,\n      ha_response: ha_response,\n      error: error,\n      processed: false\n    }\n\n    # Add to pending results\n    pending_results << result_entry\n\n    # Update conversation metadata\n    updated_metadata = existing_metadata.merge(\n      \"pending_ha_results\" => pending_results\n    )\n\n    conversation.update!(metadata_json: updated_metadata)\n\n    Rails.logger.info \"üíæ Stored HA results in conversation metadata (not as conversation log)\"\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_summarizer_job.rb`:\n\n```rb\n# app/jobs/conversation_summarizer_job.rb\n\nclass ConversationSummarizerJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üß† ConversationSummarizerJob starting\"\n\n    # Get conversations that don't already have an associated conversation summary\n    unsummarized_conversation_ids = get_unsummarized_conversations\n\n    if unsummarized_conversation_ids.any?\n      Rails.logger.info \"üìä Found #{unsummarized_conversation_ids.count} conversations to summarize\"\n      WorldStateUpdaters::ConversationSummarizerService.call(unsummarized_conversation_ids)\n    else\n      Rails.logger.info \"üò¥ No unsummarized conversations found\"\n      # Still create an empty summary for record keeping\n      WorldStateUpdaters::ConversationSummarizerService.call([])\n    end\n\n    Rails.logger.info \"‚úÖ ConversationSummarizerJob completed successfully\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå ConversationSummarizerJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\n\n  private\n\n  def get_unsummarized_conversations\n    # Get all conversation IDs that are already referenced in Summary metadata\n    summarized_ids = []\n\n    Summary.find_each do |summary|\n      metadata = summary.metadata_json\n      if metadata[\"conversation_ids\"].present?\n        summarized_ids.concat(Array(metadata[\"conversation_ids\"]))\n      end\n    end\n\n    # Get all conversation IDs that aren't in the summarized list\n    Conversation.where.not(id: summarized_ids.uniq).pluck(:id)\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/weather_forecast_summarizer_job.rb`:\n\n```rb\n# app/jobs/weather_forecast_summarizer_job.rb\n\nclass WeatherForecastSummarizerJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üå§Ô∏è WeatherForecastSummarizerJob starting\"\n\n    WorldStateUpdaters::WeatherForecastSummarizerService.call\n\n    Rails.logger.info \"‚úÖ WeatherForecastSummarizerJob completed successfully\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå WeatherForecastSummarizerJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/performance_mode_job.rb`:\n\n```rb\n# app/jobs/performance_mode_job.rb\n# Background job to handle autonomous performance mode\n\nclass PerformanceModeJob < ApplicationJob\n  queue_as :default\n\n  def perform(session_id:, performance_type:, duration_minutes:, prompt:, persona: nil)\n    Rails.logger.info \"üé™ Starting performance mode job for session #{session_id}\"\n\n    begin\n      # Create service instance\n      service = PerformanceModeService.new(\n        session_id: session_id,\n        performance_type: performance_type,\n        duration_minutes: duration_minutes,\n        prompt: prompt,\n        persona: persona\n      )\n\n      # Set running state\n      service.instance_variable_set(:@start_time, Time.current)\n      service.instance_variable_set(:@end_time, Time.current + duration_minutes.minutes)\n      service.instance_variable_set(:@is_running, true)\n      service.instance_variable_set(:@should_stop, false)\n      service.instance_variable_set(:@performance_segments, [])\n\n      # Store initial state\n      service.send(:store_performance_state)\n\n      # Run the performance loop\n      service.run_performance_loop\n\n      Rails.logger.info \"‚úÖ Performance mode job completed for session #{session_id}\"\n\n    rescue => e\n      Rails.logger.error \"‚ùå Performance mode job failed: #{e.message}\"\n      Rails.logger.error e.backtrace.first(10)\n\n      # Try to clean up state\n      begin\n        Rails.cache.delete(\"performance_mode:#{session_id}\")\n      rescue\n        # Ignore cleanup errors\n      end\n    end\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/intermediate_summarizer_job.rb`:\n\n```rb\n# frozen_string_literal: true\n\n# IntermediateSummarizerJob\n# Runs every 3 hours to create higher-level summaries of hourly summaries and goal completions\n# Creates 'intermediate' type summaries and extracts future events and key memories\nclass IntermediateSummarizerJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üß† IntermediateSummarizerJob starting\"\n\n    # Get all summaries from the last 3 hours\n    cutoff_time = 3.hours.ago\n    recent_summaries = collect_recent_summaries(cutoff_time)\n\n    if recent_summaries.any?\n      Rails.logger.info \"üìä Found #{recent_summaries.count} summaries to synthesize into intermediate summary\"\n      intermediate_summary = create_intermediate_summary(recent_summaries, cutoff_time)\n\n      # Extract future events and key memories from the intermediate summary\n      extract_memories_and_events_with_service(intermediate_summary) if intermediate_summary\n    else\n      Rails.logger.info \"üò¥ No summaries found in the last 3 hours\"\n      create_empty_intermediate_summary(cutoff_time)\n    end\n\n    Rails.logger.info \"‚úÖ IntermediateSummarizerJob completed successfully\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå IntermediateSummarizerJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\n\n  private\n\n  def collect_recent_summaries(cutoff_time)\n    # Collect hourly summaries and goal completions from the last 3 hours\n    summaries = Summary.where(\n      \"created_at >= ? AND summary_type IN (?)\",\n      cutoff_time,\n      %w[hourly goal_completion]\n    ).order(:created_at)\n\n    Rails.logger.info \"üìä Collected #{summaries.count} summaries: #{summaries.group(:summary_type).count}\"\n    summaries\n  end\n\n  def create_intermediate_summary(summaries, cutoff_time)\n    # Generate synthesis using LLM\n    synthesis_data = generate_synthesis_with_llm(summaries)\n\n    # Calculate time bounds\n    start_time = summaries.minimum(:start_time) || cutoff_time\n    end_time = Time.current\n    total_messages = summaries.sum(:message_count)\n\n    # Store intermediate summary\n    intermediate_summary = Summary.create!(\n      summary_type: \"intermediate\",\n      summary_text: synthesis_data[\"synthesis_summary\"],\n      start_time: start_time,\n      end_time: end_time,\n      message_count: total_messages,\n      metadata: {\n        general_mood: synthesis_data[\"general_mood\"],\n        key_insights: synthesis_data[\"key_insights\"],\n        important_questions: synthesis_data[\"important_questions\"],\n        goal_progress_summary: synthesis_data[\"goal_progress_summary\"],\n        future_events_detected: synthesis_data[\"future_events_detected\"] || [],\n        key_memories_detected: synthesis_data[\"key_memories_detected\"] || [],\n        period_type: \"3_hour_synthesis\",\n        source_summary_ids: summaries.pluck(:id),\n        source_summary_count: summaries.count,\n        is_intermediate: true\n      }.to_json\n    )\n\n    Rails.logger.info \"‚úÖ Created intermediate summary (ID: #{intermediate_summary.id})\"\n    intermediate_summary\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Failed to create intermediate summary: #{e.message}\"\n    nil\n  end\n\n  def generate_synthesis_with_llm(summaries)\n    prompt = build_synthesis_prompt(summaries)\n\n    response = LlmService.generate_text(\n      prompt: prompt,\n      system_prompt: build_synthesis_system_prompt,\n      model: \"google/gemini-2.5-flash\",\n      temperature: 0.4,\n      max_tokens: 2000\n    )\n\n    parse_synthesis_response(response)\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå LLM synthesis generation failed: #{e.message}\"\n    empty_synthesis\n  end\n\n  def build_synthesis_system_prompt\n    <<~PROMPT\n      You are a high-level memory synthesizer for a Burning Man AI assistant. Your job is to analyze multiple#{' '}\n      time periods and create a coherent 3-hour synthesis that captures the most important patterns and insights.\n\n      Analyze the provided summaries and extract:\n      1. **general_mood** - Overall emotional arc across the 3-hour period\n      2. **key_insights** - Most important realizations, patterns, or learnings\n      3. **important_questions** - Questions that need follow-up or show recurring themes\n      4. **goal_progress_summary** - How goals progressed or changed during this period\n      5. **future_events_detected** - Any events or activities mentioned for the future\n      6. **key_memories_detected** - Important facts, preferences, or context that should be remembered\n      7. **synthesis_summary** - A comprehensive narrative of what happened in this 3-hour window\n\n      Focus on identifying:\n      - Recurring themes and patterns\n      - Emotional progressions\n      - Goal completion and switching patterns\n      - Future planning and event mentions\n      - Important personal preferences or facts revealed\n\n      Return JSON format:\n      {\n        \"general_mood\": \"productive and exploratory\",\n        \"key_insights\": [\n          \"User is planning their first burn experience\",\n          \"Strong pattern of technical problem-solving emerged\"\n        ],\n        \"important_questions\": [\n          \"How do the art cars coordinate their routes?\",\n          \"What's the best way to find specific camps?\"\n        ],\n        \"goal_progress_summary\": \"Completed 2 exploration goals, switched to preparation mode\",\n        \"future_events_detected\": [\n          {\n            \"description\": \"Temple burn ceremony Sunday night\",\n            \"confidence\": \"high\",\n            \"timeframe\": \"Sunday evening\"\n          }\n        ],\n        \"key_memories_detected\": [\n          {\n            \"memory\": \"User prefers interactive art over passive viewing\",\n            \"type\": \"preference\",\n            \"importance\": 7\n          }\n        ],\n        \"synthesis_summary\": \"3-hour period marked by active exploration and preparation...\"\n      }\n    PROMPT\n  end\n\n  def build_synthesis_prompt(summaries)\n    goal_completions = summaries.select { |s| s.summary_type == \"goal_completion\" }\n    hourly_summaries = summaries.select { |s| s.summary_type == \"hourly\" }\n\n    <<~PROMPT\n      Synthesize the following #{summaries.count} summaries from a 3-hour period:\n\n      === HOURLY SUMMARIES (#{hourly_summaries.count}) ===\n      #{format_hourly_summaries_for_prompt(hourly_summaries)}\n\n      === GOAL COMPLETIONS (#{goal_completions.count}) ===\n      #{format_goal_completions_for_prompt(goal_completions)}\n\n      Current Context:\n      #{build_current_context}\n\n      Create a coherent synthesis that identifies patterns, emotional arcs, recurring themes,\n      and important insights from this 3-hour window. Focus on what's most significant\n      for understanding the user's experience and planning future interactions.\n\n      Pay special attention to:\n      - Any future events mentioned that should be tracked\n      - Personal preferences or facts that should be remembered\n      - Goal progression patterns\n      - Emotional or mood changes\n      - Recurring questions or interests\n    PROMPT\n  end\n\n  def format_hourly_summaries_for_prompt(summaries)\n    return \"No hourly summaries in this period.\" if summaries.empty?\n\n    summaries.map.with_index(1) do |summary, index|\n      metadata = summary.metadata_json\n\n      <<~SUMMARY\n        #{index}. #{summary.start_time&.strftime('%I:%M %p')} - #{summary.end_time&.strftime('%I:%M %p')}\n        Mood: #{metadata['general_mood'] || 'unknown'}\n        Messages: #{summary.message_count}\n        Summary: #{summary.summary_text}\n        Questions: #{(metadata['important_questions'] || []).join('; ')}\n        Thoughts: #{(metadata['useful_thoughts'] || []).join('; ')}\n        Goal Progress: #{metadata['goal_progress'] || 'unknown'}\n\n      SUMMARY\n    end.join(\"\\n\")\n  end\n\n  def format_goal_completions_for_prompt(goal_completions)\n    return \"No goals completed in this period.\" if goal_completions.empty?\n\n    goal_completions.map.with_index(1) do |completion, index|\n      metadata = completion.metadata_json\n\n      <<~COMPLETION\n        #{index}. Goal: #{completion.summary_text}\n        Category: #{metadata['goal_category'] || 'unknown'}\n        Duration: #{metadata['duration_seconds']&.to_i&./(60)&.round(1) || 'unknown'} minutes\n        Completed: #{completion.created_at.strftime('%I:%M %p')}\n        Notes: #{metadata['completion_notes'] || 'none'}\n        Expired?: #{metadata['expired'] ? 'yes' : 'no'}\n\n      COMPLETION\n    end.join(\"\\n\")\n  end\n\n  def build_current_context\n    begin\n      goal_status = GoalService.current_goal_status\n      current_goal = goal_status ? goal_status[:goal_description] : \"No active goal\"\n      safety_mode = GoalService.safety_mode_active? ? \"SAFETY MODE ACTIVE\" : \"Normal operation\"\n\n      \"Current Goal: #{current_goal} | #{safety_mode} | Time: #{Time.current.strftime('%A %I:%M %p')}\"\n    rescue StandardError => e\n      Rails.logger.error \"Failed to build current context: #{e.message}\"\n      \"Context unavailable\"\n    end\n  end\n\n  def parse_synthesis_response(response)\n    # Remove markdown code blocks if present\n    cleaned_response = response.gsub(/```json\\s*\\n?/, \"\").gsub(/```\\s*$/, \"\").strip\n\n    JSON.parse(cleaned_response)\n  rescue JSON::ParserError => e\n    Rails.logger.error \"‚ùå Failed to parse synthesis JSON: #{e.message}\"\n    Rails.logger.error \"Response was: #{response}\"\n\n    # Fallback to basic parsing if JSON fails\n    {\n      \"general_mood\" => \"unable to determine\",\n      \"key_insights\" => [ \"Failed to parse AI response\" ],\n      \"important_questions\" => [],\n      \"goal_progress_summary\" => \"unknown\",\n      \"future_events_detected\" => [],\n      \"key_memories_detected\" => [],\n      \"synthesis_summary\" => response.truncate(300)\n    }\n  end\n\n  def empty_synthesis\n    {\n      \"general_mood\" => \"quiet\",\n      \"key_insights\" => [],\n      \"important_questions\" => [],\n      \"goal_progress_summary\" => \"no activity\",\n      \"future_events_detected\" => [],\n      \"key_memories_detected\" => [],\n      \"synthesis_summary\" => \"Quiet 3-hour period with no significant activity.\"\n    }\n  end\n\n  def create_empty_intermediate_summary(cutoff_time)\n    Summary.create!(\n      summary_type: \"intermediate\",\n      summary_text: \"Quiet 3-hour period with no significant activity.\",\n      start_time: cutoff_time,\n      end_time: Time.current,\n      message_count: 0,\n      metadata: {\n        general_mood: \"quiet\",\n        key_insights: [],\n        important_questions: [],\n        goal_progress_summary: \"no activity\",\n        future_events_detected: [],\n        key_memories_detected: [],\n        period_type: \"3_hour_synthesis\",\n        source_summary_ids: [],\n        source_summary_count: 0,\n        is_intermediate: true\n      }.to_json\n    )\n\n    Rails.logger.info \"‚úÖ Created empty intermediate summary for quiet period\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Failed to create empty intermediate summary: #{e.message}\"\n  end\n\n  def extract_memories_and_events_with_service(intermediate_summary)\n    # Use the dedicated MemoryExtractionService for consistent extraction\n    extraction_results = Memory::MemoryExtractionService.call(intermediate_summary)\n\n    Rails.logger.info \"üß† MemoryExtractionService results: #{extraction_results[:events_created]} events, #{extraction_results[:memories_created]} memories\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Failed to extract memories and events with service: #{e.message}\"\n  end\n\n  def create_event_from_synthesis(event_data, summary_id)\n    return unless event_data.is_a?(Hash) && event_data[\"description\"].present?\n\n    # Parse timeframe into actual datetime if possible\n    event_time = parse_event_timeframe(event_data[\"timeframe\"])\n    return unless event_time\n\n    # Check if similar event already exists\n    existing = Event.where(\n      event_time: (event_time - 2.hours)..(event_time + 2.hours),\n      title: generate_event_title(event_data[\"description\"])\n    ).first\n\n    return if existing\n\n    Event.create!(\n      title: generate_event_title(event_data[\"description\"]),\n      description: event_data[\"description\"],\n      event_time: event_time,\n      location: event_data[\"location\"] || \"Black Rock City\",\n      importance: calculate_event_importance(event_data[\"confidence\"]),\n      extracted_from_session: \"intermediate_summary_#{summary_id}\",\n      metadata: {\n        extraction_source: \"intermediate_synthesis\",\n        confidence: event_data[\"confidence\"],\n        original_timeframe: event_data[\"timeframe\"],\n        summary_id: summary_id\n      }.to_json\n    )\n\n    Rails.logger.info \"üìÖ Created event from synthesis: #{event_data['description']}\"\n  rescue StandardError => e\n    Rails.logger.warn \"Failed to create event from synthesis: #{e.message}\"\n  end\n\n  def create_memory_from_synthesis(memory_data, summary_id)\n    return unless memory_data.is_a?(Hash) && memory_data[\"memory\"].present?\n\n    memory_type = memory_data[\"type\"] || \"context\"\n    importance = memory_data[\"importance\"] || 5\n\n    # Validate memory type\n    return unless ConversationMemory::MEMORY_TYPES.include?(memory_type)\n\n    ConversationMemory.create!(\n      session_id: \"intermediate_synthesis_#{summary_id}\",\n      summary: memory_data[\"memory\"],\n      memory_type: memory_type,\n      importance: [ importance.to_i, 10 ].min, # Cap at 10\n      metadata: {\n        extraction_source: \"intermediate_synthesis\",\n        summary_id: summary_id,\n        original_context: memory_data[\"context\"]\n      }.to_json\n    )\n\n    Rails.logger.info \"üß† Created memory from synthesis: #{memory_data['memory']}\"\n  rescue StandardError => e\n    Rails.logger.warn \"Failed to create memory from synthesis: #{e.message}\"\n  end\n\n  def parse_event_timeframe(timeframe_str)\n    return nil unless timeframe_str.present?\n\n    # Simple parsing for common patterns\n    base_time = Time.current\n\n    case timeframe_str.downcase\n    when /tonight/\n      base_time.end_of_day - 2.hours # 10 PM tonight\n    when /tomorrow.*morning/\n      (base_time + 1.day).beginning_of_day + 10.hours # 10 AM tomorrow\n    when /tomorrow.*evening/, /tomorrow.*night/\n      (base_time + 1.day).beginning_of_day + 20.hours # 8 PM tomorrow\n    when /sunday.*evening/, /sunday.*night/\n      next_sunday = base_time.next_occurring(:sunday)\n      next_sunday.beginning_of_day + 20.hours # 8 PM next Sunday\n    when /(\\d{1,2}):(\\d{2})\\s*(am|pm)/\n      # Extract time and assume today or tomorrow\n      hour = $1.to_i\n      minute = $2.to_i\n      meridiem = $3.downcase\n\n      hour += 12 if meridiem == \"pm\" && hour != 12\n      hour = 0 if meridiem == \"am\" && hour == 12\n\n      event_time = base_time.beginning_of_day + hour.hours + minute.minutes\n      event_time += 1.day if event_time < base_time # If time has passed, assume tomorrow\n\n      event_time\n    else\n      # Default to tomorrow evening if can't parse\n      (base_time + 1.day).beginning_of_day + 20.hours\n    end\n  rescue StandardError\n    nil\n  end\n\n  def generate_event_title(description)\n    # Generate a concise title from description\n    return \"Extracted Event\" if description.blank?\n\n    # Take first few words, clean up\n    words = description.split(/\\s+/).take(5)\n    title = words.join(\" \")\n    title = title.gsub(/[.!?]+$/, \"\") # Remove trailing punctuation\n    title.length > 40 ? \"#{title[0..37]}...\" : title\n  end\n\n  def calculate_event_importance(confidence)\n    case confidence&.downcase\n    when \"high\" then 8\n    when \"medium\" then 6\n    when \"low\" then 4\n    else 5\n    end\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_memory_job.rb`:\n\n```rb\n# frozen_string_literal: true\n\nclass ConversationMemoryJob < ApplicationJob\n  queue_as :default\n\n  def perform(session_id)\n    Rails.logger.info \"üß† Creating memories for session: #{session_id}\"\n\n    conversation = Conversation.find_by(session_id: session_id)\n    return unless conversation&.ended_at\n\n    # Only create memories for conversations with multiple messages\n    logs = conversation.conversation_logs.order(:created_at)\n    return if logs.count < 2\n\n    # Get environmental context\n    context = fetch_environmental_context\n\n    # Extract memorable insights from the conversation\n    memories = extract_conversation_memories(conversation, logs, context)\n\n    # Create ConversationMemory records\n    memories.each do |memory_data|\n      ConversationMemory.create!(\n        session_id: session_id,\n        summary: memory_data[:summary],\n        memory_type: memory_data[:type],\n        importance: memory_data[:importance],\n        metadata: memory_data[:metadata].to_json\n      )\n    end\n\n    Rails.logger.info \"‚úÖ Created #{memories.count} memories for session: #{session_id}\"\n  end\n\n  private\n\n  def hass\n   @hass ||= HomeAssistantService\n  end\n\n  def fetch_environmental_context\n    context = hass.entity(\"sensor.world_state\")\n    context += hass.entity(\"sensor.glitchcube_context\")\n  end\n\n  def extract_conversation_memories(conversation, logs, context)\n    memories = []\n\n    # Extract all conversation text for analysis\n    conversation_text = logs.map { |log|\n      \"#{log.user_message}\\n#{log.ai_response}\"\n    }.join(\"\\n\")\n\n    # Extract locations mentioned in conversation\n    extracted_locations = extract_locations(conversation_text, context)\n\n    # Extract upcoming events from conversation\n    extracted_events = extract_events(conversation_text, conversation.session_id, context)\n\n    # Create conversation memory for significant interactions\n    if logs.count >= 3\n      event_summary = summarize_interaction(conversation, logs, context, extracted_locations)\n      memories << {\n        summary: event_summary,\n        type: \"event\",\n        importance: calculate_basic_importance(logs),\n        metadata: {\n          extracted_at: Time.current,\n          context: context,\n          persona: conversation.persona,\n          message_count: logs.count,\n          duration: calculate_duration(logs),\n          locations: extracted_locations,\n          events_mentioned: extracted_events.size\n        }\n      }\n    end\n\n    # Create Event records for upcoming events\n    extracted_events.each do |event_data|\n      create_event_record(event_data)\n    end\n\n    memories\n  end\n\n  def extract_preferences(user_messages, ai_responses)\n    # Basic implementation - just store that a conversation happened\n    # TODO: Implement proper LLM-based memory extraction later\n    []\n  end\n\n  def has_high_engagement(logs)\n    # Consider high engagement if:\n    # - Long messages (avg > 50 chars)\n    # - Tools were used\n    # - Multiple back-and-forth exchanges\n\n    avg_length = logs.map { |log| log.user_message.length }.sum / logs.count.to_f\n    has_tools = logs.any? { |log| log.ai_response.include?(\"[THOUGHTS:\") }\n\n    avg_length > 50 || has_tools || logs.count >= 3\n  end\n\n  def summarize_interaction(conversation, logs, context, locations = [])\n    first_message = logs.first.user_message.truncate(100)\n    persona = conversation.persona.capitalize\n\n    summary = \"#{context[:time_of_day].capitalize} interaction with #{persona} persona\"\n\n    # Add specific locations if mentioned\n    if locations.any?\n      summary += \" discussing #{locations.join(', ')}\"\n    elsif context[:location] != \"Black Rock City\"\n      summary += \" at #{context[:location]}\"\n    end\n\n    summary += \". Started with: \\\"#{first_message}\\\"\"\n\n    if logs.count >= 5\n      summary += \". Extended #{logs.count}-message conversation\"\n    end\n\n    summary\n  end\n\n  def calculate_basic_importance(logs)\n    # Simple importance based on conversation length\n    case logs.count\n    when 3..4 then 5\n    when 5..9 then 6\n    when 10..20 then 7\n    else 8\n    end\n  end\n\n  def calculate_duration(logs)\n    return 0 if logs.count < 2\n\n    start_time = logs.first.created_at\n    end_time = logs.last.created_at\n    (end_time - start_time).to_i\n  end\n\n  def extract_locations(conversation_text, context)\n    # Simple keyword-based location extraction\n    locations = []\n\n    # Common Burning Man locations\n    burning_man_locations = [\n      \"Center Camp\", \"Man\", \"Temple\", \"Esplanade\", \"Playa\", \"Deep Playa\",\n      \"Exodus\", \"Gate\", \"Airport\", \"Rangers\", \"DMV\", \"Will Call\",\n      \"Arctica\", \"Ice\", \"Trash Fence\", \"Desert\", \"Dust Storm\"\n    ]\n\n    # Check for specific locations mentioned\n    burning_man_locations.each do |location|\n      if conversation_text.downcase.include?(location.downcase)\n        locations << location\n      end\n    end\n\n    # Extract camp names (simple pattern matching)\n    camp_matches = conversation_text.scan(/(?:at|near|by)\\s+([A-Z][a-zA-Z\\s]+(?:Camp|Village|Plaza))/i)\n    locations.concat(camp_matches.flatten.map(&:strip))\n\n    # Extract street addresses (like 6:00 and Esplanade)\n    street_matches = conversation_text.scan(/(\\d{1,2}:\\d{2}(?:\\s+and\\s+\\w+)?)/i)\n    locations.concat(street_matches.flatten)\n\n    locations.uniq.take(3) # Limit to avoid noise\n  end\n\n  def extract_events(conversation_text, session_id, context)\n    events = []\n\n    # Simple pattern matching for events with times\n    event_patterns = [\n      # \"X at location tomorrow at 3pm\" or \"X tomorrow at 3pm\"\n      /([^.!?]{10,80}?)\\s+(?:tomorrow|today|tonight)\\s+at\\s+(\\d{1,2}(?::\\d{2})?\\s*(?:am|pm|AM|PM)?)/i,\n      # \"X happening at Y time\"\n      /([^.!?]{10,50})\\s+(?:happening|starting|beginning)\\s+(?:at\\s+)?(\\d{1,2}(?::\\d{2})?\\s*(?:am|pm|AM|PM)?)/i,\n      # \"X is at Y time\"\n      /([^.!?]{10,50})\\s+is\\s+at\\s+(\\d{1,2}(?::\\d{2})?\\s*(?:am|pm|AM|PM)?)/i,\n      # \"there is/will be X at time\"\n      /(?:there\\s+(?:is|will\\s+be))\\s+([^.!?]{10,80}?)\\s+at\\s+(\\d{1,2}(?::\\d{2})?\\s*(?:am|pm|AM|PM)?)/i\n    ]\n\n    event_patterns.each do |pattern|\n      conversation_text.scan(pattern) do |match|\n        if match.length >= 2\n          description = match[0]&.strip\n          time_str = match[1]&.strip\n\n          next if description.blank? || time_str.blank?\n\n          # Parse the time (simple approach)\n          event_time = parse_event_time(time_str, context)\n          next unless event_time\n\n          # Extract location from description if possible\n          location = extract_location_from_description(description) || context[:location]\n\n          events << {\n            title: generate_event_title(description),\n            description: description,\n            event_time: event_time,\n            location: location,\n            importance: 6, # Medium importance for extracted events\n            session_id: session_id,\n            metadata: {\n              extracted_from: \"conversation\",\n              raw_text: \"#{description} #{time_str}\",\n              context: context\n            }\n          }\n        end\n      end\n    end\n\n    # Remove duplicates by similar titles and same time\n    unique_events = []\n    events.each do |event|\n      unless unique_events.any? { |existing|\n        similar_titles?(existing[:title], event[:title]) &&\n        (existing[:event_time] - event[:event_time]).abs < 30.minutes\n      }\n        unique_events << event\n      end\n    end\n\n    unique_events.take(3) # Limit results\n  end\n\n  def parse_event_time(time_str, context)\n    return nil if time_str.blank?\n\n    # Simple time parsing - assume today/tomorrow\n    base_date = Date.current\n\n    # Parse hour and minute\n    if time_str.match(/(\\d{1,2})(?::(\\d{2}))?\\s*(am|pm|AM|PM)?/i)\n      hour = $1.to_i\n      minute = $2&.to_i || 0\n      meridiem = $3&.downcase\n\n      # Convert to 24-hour format\n      if meridiem == \"pm\" && hour != 12\n        hour += 12\n      elsif meridiem == \"am\" && hour == 12\n        hour = 0\n      end\n\n      # If the time has passed today, assume tomorrow\n      event_time = base_date.beginning_of_day + hour.hours + minute.minutes\n      event_time += 1.day if event_time < Time.current\n\n      event_time\n    end\n  rescue StandardError\n    nil\n  end\n\n  def extract_location_from_description(description)\n    # Simple location extraction from event description\n    return nil if description.blank?\n\n    # Look for \"at [location]\" pattern, but exclude time words\n    if description.match(/\\bat\\s+([A-Z][a-zA-Z\\s]+?)(?:\\s+(?:tomorrow|today|tonight|at\\s+\\d)|\\s*$)/i)\n      location = $1.strip\n      # Remove \"tomorrow\", \"today\", etc. if they snuck in\n      location.gsub(/\\b(?:tomorrow|today|tonight)\\b/i, \"\").strip\n    end\n  end\n\n  def similar_titles?(title1, title2)\n    # Simple similarity check - if one title contains most words of the other\n    return false if title1.blank? || title2.blank?\n\n    words1 = title1.downcase.split(/\\s+/)\n    words2 = title2.downcase.split(/\\s+/)\n\n    shorter, longer = [ words1, words2 ].sort_by(&:length)\n    common_words = shorter & longer\n\n    # Consider similar if 70% of shorter title words are in longer title\n    common_words.length.to_f / shorter.length > 0.7\n  end\n\n  def generate_event_title(description)\n    # Generate a concise title from description\n    return \"Untitled Event\" if description.blank?\n\n    # Take first few words, clean up\n    words = description.split(/\\s+/).take(6)\n    title = words.join(\" \")\n    title = title.gsub(/[.!?]+$/, \"\") # Remove trailing punctuation\n    title.length > 50 ? \"#{title[0..47]}...\" : title\n  end\n\n  def create_event_record(event_data)\n    return if event_data[:event_time].blank? || event_data[:description].blank?\n\n    # Check if similar event already exists\n    existing = Event.where(\n      event_time: (event_data[:event_time] - 1.hour)..(event_data[:event_time] + 1.hour),\n      title: event_data[:title]\n    ).first\n\n    return if existing\n\n    Event.create!(\n      title: event_data[:title],\n      description: event_data[:description],\n      event_time: event_data[:event_time],\n      location: event_data[:location],\n      importance: event_data[:importance],\n      extracted_from_session: event_data[:session_id],\n      metadata: event_data[:metadata].to_json\n    )\n\n    Rails.logger.info \"üìÖ Created event: #{event_data[:title]} at #{event_data[:event_time]}\"\n  rescue StandardError => e\n    Rails.logger.warn \"Failed to create event: #{e.message}\"\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/gps_sensor_update_job.rb`:\n\n```rb\n# frozen_string_literal: true\n\n# Background job to update Home Assistant with current GPS location\n# Runs every 5 minutes to keep the sensor data fresh\nclass GpsSensorUpdateJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    begin\n      # Get current location from GPS service\n      gps_service = Services::Gps::GPSTrackingService.new\n      location_data = gps_service.current_location\n      Rails.logger.info(\"***LOCATION #{location_data}***\")\n      return unless location_data && location_data[:lat] && location_data[:lng]\n\n      # Update Home Assistant location context sensor only\n      # (GPS coordinates come from HA device tracker)\n      ha_service = HomeAssistantService.new\n\n      # Create location context sensor with enriched data\n      ha_service.set_entity_state(\n        \"sensor.glitchcube_location_context\",\n        location_data[:address] || location_data[:zone]&.to_s&.humanize || \"Unknown\",\n        {\n          friendly_name: \"GlitchCube Location Context\",\n          icon: \"mdi:map-marker-radius\",\n\n          # Location details\n          zone: location_data[:zone],\n          address: location_data[:address],\n          street: location_data[:street],\n          block: location_data[:block],\n\n          # Geofencing\n          within_fence: location_data[:within_fence],\n          distance_from_man: location_data[:distance_from_man],\n\n          # Landmarks and POIs\n          landmarks: location_data[:landmarks]&.first(10)&.uniq&.last(5)&.map { |l| l[:name] }&.join(\", \"),\n          landmark_count: location_data[:landmarks]&.count || 0,\n          nearest_landmark: location_data[:landmarks]&.first&.[](:name),\n\n          # Porto info\n          nearest_porto: location_data[:nearest_porto]&.[](:name),\n          porto_distance: location_data[:nearest_porto]&.[](:distance_meters),\n\n          # Metadata\n          coordinates: \"#{location_data[:lat]}, #{location_data[:lng]}\",\n          source: location_data[:source] || \"home_assistant\",\n          last_updated: Time.now.iso8601\n        }\n      )\n\n      Rails.logger.info \"GPS sensor update completed successfully\"\n    rescue StandardError => e\n      Rails.logger.error \"GPS sensor update failed: #{e.message}\"\n      # Don't re-raise - we don't want to break the job queue\n    end\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/daily_summarizer_job.rb`:\n\n```rb\n# frozen_string_literal: true\n\n# DailySummarizerJob\n# Runs once per day to create comprehensive daily summaries from all activity\n# Synthesizes hourly, intermediate (3-hour), and goal completion summaries\nclass DailySummarizerJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üìÖ DailySummarizerJob starting\"\n\n    # Get all summaries from the last 24 hours\n    cutoff_time = 24.hours.ago\n    daily_summaries = collect_daily_summaries(cutoff_time)\n\n    if daily_summaries.any?\n      Rails.logger.info \"üìä Found #{daily_summaries.count} summaries to synthesize into daily summary\"\n      create_daily_summary(daily_summaries, cutoff_time)\n    else\n      Rails.logger.info \"üò¥ No summaries found in the last 24 hours\"\n      create_empty_daily_summary(cutoff_time)\n    end\n\n    Rails.logger.info \"‚úÖ DailySummarizerJob completed successfully\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå DailySummarizerJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\n\n  private\n\n  def collect_daily_summaries(cutoff_time)\n    # Collect all relevant summaries from the last 24 hours\n    summaries = Summary.where(\n      \"created_at >= ? AND summary_type IN (?)\",\n      cutoff_time,\n      %w[hourly intermediate goal_completion]\n    ).order(:created_at)\n\n    summary_counts = summaries.group(:summary_type).count\n    Rails.logger.info \"üìä Collected #{summaries.count} summaries: #{summary_counts}\"\n\n    summaries\n  end\n\n  def create_daily_summary(summaries, cutoff_time)\n    # Generate daily synthesis using LLM\n    daily_synthesis = generate_daily_synthesis_with_llm(summaries)\n\n    # Calculate time bounds and metrics\n    start_time = summaries.minimum(:start_time) || cutoff_time\n    end_time = Time.current\n    total_messages = summaries.sum(:message_count)\n\n    # Store daily summary\n    daily_summary = Summary.create!(\n      summary_type: \"daily\",\n      summary_text: daily_synthesis[\"daily_summary\"],\n      start_time: start_time,\n      end_time: end_time,\n      message_count: total_messages,\n      metadata: {\n        overall_mood: daily_synthesis[\"overall_mood\"],\n        daily_themes: daily_synthesis[\"daily_themes\"],\n        goal_completion_analysis: daily_synthesis[\"goal_completion_analysis\"],\n        key_insights: daily_synthesis[\"key_insights\"],\n        emotional_arc: daily_synthesis[\"emotional_arc\"],\n        productivity_assessment: daily_synthesis[\"productivity_assessment\"],\n        recurring_patterns: daily_synthesis[\"recurring_patterns\"],\n        important_questions: daily_synthesis[\"important_questions\"],\n        period_type: \"daily_synthesis\",\n        source_summary_ids: summaries.pluck(:id),\n        source_breakdown: summaries.group(:summary_type).count,\n        total_conversations: calculate_conversation_count(summaries),\n        peak_activity_periods: daily_synthesis[\"peak_activity_periods\"] || []\n      }.to_json\n    )\n\n    Rails.logger.info \"‚úÖ Created daily summary (ID: #{daily_summary.id}) covering #{total_messages} messages\"\n    daily_summary\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Failed to create daily summary: #{e.message}\"\n    nil\n  end\n\n  def generate_daily_synthesis_with_llm(summaries)\n    prompt = build_daily_synthesis_prompt(summaries)\n\n    response = LlmService.generate_text(\n      prompt: prompt,\n      system_prompt: build_daily_synthesis_system_prompt,\n      model: \"google/gemini-2.5-flash\",\n      temperature: 0.3, # Lower temperature for more consistent daily analysis\n      max_tokens: 3000\n    )\n\n    parse_daily_synthesis_response(response)\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Daily LLM synthesis generation failed: #{e.message}\"\n    empty_daily_synthesis\n  end\n\n  def build_daily_synthesis_system_prompt\n    <<~PROMPT\n      You are a comprehensive daily memory synthesizer for a Burning Man AI assistant. Your job is to analyze#{' '}\n      a full day's worth of activity and create a coherent narrative that captures the most important patterns,#{' '}\n      emotional arcs, and insights from the 24-hour period.\n\n      Analyze the provided summaries and extract:\n      1. **overall_mood** - The dominant emotional tone across the entire day\n      2. **daily_themes** - Major recurring themes or topics that defined the day\n      3. **goal_completion_analysis** - How goals were progressed, completed, or changed throughout the day\n      4. **key_insights** - Most important realizations or learnings from the day\n      5. **emotional_arc** - How emotions and energy levels changed throughout the day\n      6. **productivity_assessment** - Overall assessment of how productive/engaging the day was\n      7. **recurring_patterns** - Patterns in behavior, interests, or interactions\n      8. **important_questions** - Questions that emerged and may need future attention\n      9. **peak_activity_periods** - When the most significant interactions or activities occurred\n      10. **daily_summary** - A comprehensive narrative of the day's events and significance\n\n      Focus on creating a coherent story of the day that would help the AI understand:\n      - What kind of day it was overall\n      - What the user was focused on or working toward\n      - How their mood and energy evolved\n      - What they learned or discovered\n      - What patterns emerged in their behavior or interests\n\n      Return JSON format:\n      {\n        \"overall_mood\": \"exploratory and engaged with periods of focused problem-solving\",\n        \"daily_themes\": [\n          \"Art installation planning and logistics\",\n          \"Community building and camp coordination\",\n          \"Technical troubleshooting and learning\"\n        ],\n        \"goal_completion_analysis\": \"Completed 4 out of 6 goals, switched focus twice due to emerging priorities\",\n        \"key_insights\": [\n          \"User shows strong preference for hands-on learning over theoretical planning\",\n          \"Most productive during morning hours, more social in evening\"\n        ],\n        \"emotional_arc\": \"Started curious and energetic, hit some frustration mid-day during technical issues, ended satisfied and reflective\",\n        \"productivity_assessment\": \"highly productive\",\n        \"recurring_patterns\": [\n          \"Tendency to dive deep into technical details\",\n          \"Regular check-ins with camp coordination\"\n        ],\n        \"important_questions\": [\n          \"How to balance individual projects with camp responsibilities?\",\n          \"What backup plans are needed for weather contingencies?\"\n        ],\n        \"peak_activity_periods\": [\n          \"9-11 AM: Intense planning session\",\n          \"6-8 PM: Community coordination activities\"\n        ],\n        \"daily_summary\": \"A day marked by productive exploration and community engagement...\"\n      }\n    PROMPT\n  end\n\n  def build_daily_synthesis_prompt(summaries)\n    hourly_summaries = summaries.select { |s| s.summary_type == \"hourly\" }\n    intermediate_summaries = summaries.select { |s| s.summary_type == \"intermediate\" }\n    goal_completions = summaries.select { |s| s.summary_type == \"goal_completion\" }\n\n    <<~PROMPT\n      Create a comprehensive daily synthesis from these #{summaries.count} summaries spanning 24 hours:\n\n      === INTERMEDIATE SUMMARIES (3-hour windows) - #{intermediate_summaries.count} ===\n      #{format_intermediate_summaries_for_prompt(intermediate_summaries)}\n\n      === HOURLY SUMMARIES - #{hourly_summaries.count} ===\n      #{format_hourly_summaries_for_daily_prompt(hourly_summaries)}\n\n      === GOAL COMPLETIONS - #{goal_completions.count} ===\n      #{format_goal_completions_for_daily_prompt(goal_completions)}\n\n      === DAILY CONTEXT ===\n      Date: #{Date.current.strftime('%A, %B %d, %Y')}\n      Total Messages: #{summaries.sum(:message_count)}\n      Activity Span: #{format_activity_timespan(summaries)}\n\n      Create a coherent daily narrative that captures:\n      - The overall character and significance of this day\n      - Major themes and patterns that emerged\n      - How goals, mood, and energy evolved throughout the day\n      - Key insights and learnings\n      - What made this day unique or noteworthy\n      - Patterns in behavior, interests, and productivity\n\n      This summary will be used to understand the user's daily rhythms, preferences,#{' '}\n      and progress toward their goals and projects.\n    PROMPT\n  end\n\n  def format_intermediate_summaries_for_prompt(summaries)\n    return \"No 3-hour synthesis summaries available.\" if summaries.empty?\n\n    summaries.map.with_index(1) do |summary, index|\n      metadata = summary.metadata_json\n\n      <<~INTERMEDIATE\n        #{index}. #{format_time_window(summary)} (#{summary.message_count} messages)\n        Mood: #{metadata['general_mood'] || 'unknown'}\n        Key Insights: #{(metadata['key_insights'] || []).join('; ')}\n        Goal Progress: #{metadata['goal_progress_summary'] || 'unknown'}\n        Summary: #{summary.summary_text}\n        Future Events Detected: #{metadata['future_events_detected']&.count || 0}\n        Key Memories Detected: #{metadata['key_memories_detected']&.count || 0}\n\n      INTERMEDIATE\n    end.join(\"\\n\")\n  end\n\n  def format_hourly_summaries_for_daily_prompt(summaries)\n    return \"No hourly summaries available (covered by intermediate summaries).\" if summaries.empty?\n\n    # Group hourly summaries by time periods for easier reading\n    grouped_summaries = summaries.group_by { |s| s.start_time&.hour&./ 3 } # Group by 3-hour blocks\n\n    grouped_summaries.map do |block_key, block_summaries|\n      start_hour = (block_key || 0) * 3\n\n      <<~HOURLY_BLOCK\n        === #{start_hour}:00 - #{start_hour + 3}:00 Block (#{block_summaries.count} hourly summaries) ===\n        #{block_summaries.map { |s| \"‚Ä¢ #{s.summary_text}\" }.join(\"\\n\")}\n\n      HOURLY_BLOCK\n    end.join(\"\\n\")\n  end\n\n  def format_goal_completions_for_daily_prompt(goal_completions)\n    return \"No goals completed today.\" if goal_completions.empty?\n\n    # Group by category for better analysis\n    categorized = goal_completions.group_by { |gc| gc.metadata_json[\"goal_category\"] || \"unknown\" }\n\n    result = []\n    categorized.each do |category, completions|\n      result << \"=== #{category.humanize} Goals (#{completions.count}) ===\"\n      completions.each_with_index do |completion, index|\n        metadata = completion.metadata_json\n        duration_text = metadata[\"duration_seconds\"] ? \"#{(metadata['duration_seconds'].to_i / 60).round(1)} min\" : \"unknown duration\"\n\n        result << \"#{index + 1}. #{completion.summary_text} (#{duration_text})\"\n        result << \"   Completed: #{completion.created_at.strftime('%I:%M %p')}\"\n        result << \"   Notes: #{metadata['completion_notes'] || 'none'}\"\n        result << \"\"\n      end\n    end\n\n    result.join(\"\\n\")\n  end\n\n  def format_time_window(summary)\n    start_str = summary.start_time&.strftime(\"%I:%M %p\") || \"unknown\"\n    end_str = summary.end_time&.strftime(\"%I:%M %p\") || \"unknown\"\n    \"#{start_str} - #{end_str}\"\n  end\n\n  def format_activity_timespan(summaries)\n    earliest = summaries.minimum(:start_time)\n    latest = summaries.maximum(:end_time)\n\n    return \"unknown\" unless earliest && latest\n\n    duration_hours = ((latest - earliest) / 1.hour).round(1)\n    \"#{earliest.strftime('%I:%M %p')} - #{latest.strftime('%I:%M %p')} (#{duration_hours}h)\"\n  end\n\n  def calculate_conversation_count(summaries)\n    # Estimate conversation count from hourly summaries\n    hourly_summaries = summaries.select { |s| s.summary_type == \"hourly\" }\n\n    # Each hourly summary typically represents multiple conversations\n    # Use metadata if available, otherwise estimate\n    hourly_summaries.sum do |summary|\n      metadata = summary.metadata_json\n      metadata[\"conversations_count\"] || (summary.message_count > 0 ? 1 : 0)\n    end\n  end\n\n  def parse_daily_synthesis_response(response)\n    # Remove markdown code blocks if present\n    cleaned_response = response.gsub(/```json\\s*\\n?/, \"\").gsub(/```\\s*$/, \"\").strip\n\n    JSON.parse(cleaned_response)\n  rescue JSON::ParserError => e\n    Rails.logger.error \"‚ùå Failed to parse daily synthesis JSON: #{e.message}\"\n    Rails.logger.error \"Response was: #{response}\"\n\n    # Fallback to basic parsing if JSON fails\n    {\n      \"overall_mood\" => \"unable to determine\",\n      \"daily_themes\" => [ \"Failed to parse AI response\" ],\n      \"goal_completion_analysis\" => \"unknown\",\n      \"key_insights\" => [],\n      \"emotional_arc\" => \"unknown\",\n      \"productivity_assessment\" => \"unknown\",\n      \"recurring_patterns\" => [],\n      \"important_questions\" => [],\n      \"peak_activity_periods\" => [],\n      \"daily_summary\" => response.truncate(400)\n    }\n  end\n\n  def empty_daily_synthesis\n    {\n      \"overall_mood\" => \"quiet\",\n      \"daily_themes\" => [ \"minimal activity\" ],\n      \"goal_completion_analysis\" => \"no goals completed\",\n      \"key_insights\" => [],\n      \"emotional_arc\" => \"stable and quiet\",\n      \"productivity_assessment\" => \"inactive\",\n      \"recurring_patterns\" => [],\n      \"important_questions\" => [],\n      \"peak_activity_periods\" => [],\n      \"daily_summary\" => \"A quiet day with minimal activity or interaction.\"\n    }\n  end\n\n  def create_empty_daily_summary(cutoff_time)\n    Summary.create!(\n      summary_type: \"daily\",\n      summary_text: \"A quiet day with minimal activity or interaction.\",\n      start_time: cutoff_time,\n      end_time: Time.current,\n      message_count: 0,\n      metadata: {\n        overall_mood: \"quiet\",\n        daily_themes: [ \"minimal activity\" ],\n        goal_completion_analysis: \"no goals completed\",\n        key_insights: [],\n        emotional_arc: \"stable and quiet\",\n        productivity_assessment: \"inactive\",\n        recurring_patterns: [],\n        important_questions: [],\n        peak_activity_periods: [],\n        period_type: \"daily_synthesis\",\n        source_summary_ids: [],\n        source_breakdown: {},\n        total_conversations: 0\n      }.to_json\n    )\n\n    Rails.logger.info \"‚úÖ Created empty daily summary for quiet day\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Failed to create empty daily summary: #{e.message}\"\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/async_tool_job.rb`:\n\n```rb\n# app/jobs/async_tool_job.rb\nrequire \"mission_control/jobs\" if defined?(Rails)\n\nclass AsyncToolJob < ApplicationJob\n  queue_as :default\n\n  def perform(validated_tool_call_or_legacy_tool_name, legacy_arguments_or_session_id = nil, session_id = nil, conversation_id = nil)\n    # Handle both ValidatedToolCall objects and legacy parameter format\n    if validated_tool_call_or_legacy_tool_name.is_a?(ValidatedToolCall)\n      validated_tool_call = validated_tool_call_or_legacy_tool_name\n      session_id = legacy_arguments_or_session_id # session_id is second parameter\n      tool_name = validated_tool_call.name\n\n      Rails.logger.info \"üîß AsyncToolJob starting with ValidatedToolCall: #{tool_name} for session: #{session_id}\"\n    else\n      # Legacy format: tool_name, tool_arguments, session_id, conversation_id\n      tool_name = validated_tool_call_or_legacy_tool_name\n      tool_arguments = legacy_arguments_or_session_id\n\n      Rails.logger.info \"üîß AsyncToolJob starting (legacy): #{tool_name} for session: #{session_id}\"\n      Rails.logger.info \"üìù Arguments received: #{tool_arguments.inspect}\"\n\n      # Clean up ActiveJob's serialization artifacts\n      cleaned_arguments = clean_activejob_keys(tool_arguments)\n      Rails.logger.info \"üßπ Cleaned arguments: #{cleaned_arguments.inspect}\"\n\n      # Create ValidatedToolCall from legacy parameters\n      validated_tool_call = create_validated_tool_call_from_legacy(tool_name, cleaned_arguments)\n    end\n\n    # Execute the tool with validation and timing\n    executor = ToolExecutor.new\n    Rails.logger.info \"‚öôÔ∏è Calling executor.execute_single_async with ValidatedToolCall\"\n    result = executor.execute_single_async(validated_tool_call)\n\n    # Log the result\n    Rails.logger.info \"‚úÖ AsyncToolJob result: #{result.inspect}\"\n    if result[:success]\n      Rails.logger.info \"üéâ Async tool #{validated_tool_call.name} completed successfully\"\n    else\n      Rails.logger.error \"‚ùå Async tool #{validated_tool_call.name} failed: #{result[:error]}\"\n    end\n\n    # Store result if we have a way to track it\n    if session_id\n      store_tool_result(validated_tool_call.name, result, session_id, conversation_id)\n    end\n\n    # Handle any follow-up actions based on result\n    handle_tool_result(validated_tool_call.name, result, session_id, conversation_id)\n\n    result\n  rescue StandardError => e\n    tool_name = validated_tool_call_or_legacy_tool_name.is_a?(ValidatedToolCall) ?\n      validated_tool_call_or_legacy_tool_name.name : validated_tool_call_or_legacy_tool_name\n\n    Rails.logger.error \"AsyncToolJob failed for #{tool_name}: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n\n    # Record the failure in metrics\n    ToolMetrics.record(tool_name: tool_name, duration_ms: 0, success: false)\n\n    error_result = {\n      success: false,\n      error: \"Job execution failed: #{e.message}\",\n      tool: tool_name\n    }\n\n    store_tool_result(tool_name, error_result, session_id, conversation_id) if session_id\n\n    error_result\n  end\n\n  private\n\n  # Clean ActiveJob serialization artifacts\n  def clean_activejob_keys(data)\n    case data\n    when Hash\n      data.reject { |k, v| k == \"_aj_symbol_keys\" }.transform_values { |v| clean_activejob_keys(v) }\n    when Array\n      data.map { |item| clean_activejob_keys(item) }\n    else\n      data\n    end\n  end\n\n  # Create ValidatedToolCall from legacy tool_name + arguments\n  def create_validated_tool_call_from_legacy(tool_name, arguments)\n    tool_call_data = {\n      \"id\" => \"async_#{SecureRandom.uuid}\",\n      \"type\" => \"function\",\n      \"function\" => {\n        \"name\" => tool_name,\n        \"arguments\" => arguments.to_json\n      }\n    }\n\n    tool_class = Tools::Registry.get_tool(tool_name)\n    ValidatedToolCall.from_tool_call_data(tool_call_data, tool_class)\n  end\n\n  def store_tool_result(tool_name, result, session_id, conversation_id)\n    Rails.logger.info \"Tool result for #{session_id}: #{tool_name} - #{result[:success] ? 'success' : 'failed'}\"\n\n    # Store as a system message in ConversationLog for context\n    ConversationLog.create!(\n      session_id: session_id,\n      user_message: \"[SYSTEM] Async tool completed: #{tool_name}\",\n      ai_response: \"[SYSTEM] #{format_tool_result(result)}\",\n      tool_results: { tool_name => result }.to_json,\n      metadata: {\n        message_type: \"async_tool_result\",\n        original_conversation_id: conversation_id,\n        tool_name: tool_name,\n        executed_at: Time.current.iso8601\n      }.to_json\n    )\n  rescue StandardError => e\n    Rails.logger.error \"Failed to store tool result: #{e.message}\"\n  end\n\n  def format_tool_result(result)\n    if result[:success]\n      \"‚úÖ Completed: #{result[:message] || 'Success'}\"\n    else\n      \"‚ùå Failed: #{result[:error] || 'Unknown error'}\"\n    end\n  end\n\n  def handle_tool_result(tool_name, result, session_id, conversation_id)\n    # Handle specific tool results that might trigger follow-up actions\n    case tool_name\n    when \"turn_on_light\", \"turn_off_light\", \"set_light_color_and_brightness\", \"set_light_effect\"\n      # Light control tools - could trigger status updates or notifications\n      handle_light_control_result(result, session_id)\n    when \"call_hass_service\"\n      # General service calls - could have various follow-up actions\n      handle_service_call_result(result, session_id)\n    end\n  end\n\n  def handle_light_control_result(result, session_id)\n    # Could trigger:\n    # - Status update broadcasts\n    # - Confirmation notifications\n    # - Integration with other systems\n    Rails.logger.debug \"Light control completed for session #{session_id}: #{result[:success]}\"\n  end\n\n  def handle_service_call_result(result, session_id)\n    # Could trigger different actions based on the service called\n    Rails.logger.debug \"Service call completed for session #{session_id}: #{result[:success]}\"\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_timeout_monitor_job.rb`:\n\n```rb\n# app/jobs/conversation_timeout_monitor_job.rb\n\nclass ConversationTimeoutMonitorJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üïê ConversationTimeoutMonitorJob starting\"\n\n    timeout_threshold = 5.minutes.ago\n\n    conversations_to_end = Conversation.active\n      .joins(:conversation_logs)\n      .where(conversation_logs: { created_at: ..timeout_threshold })\n      .group(\"conversations.id\")\n      .having(\"MAX(conversation_logs.created_at) < ?\", timeout_threshold)\n\n    conversations_to_end.find_each do |conversation|\n      Rails.logger.info \"‚è∞ Ending conversation #{conversation.session_id} (last activity: #{conversation.conversation_logs.recent.first&.created_at})\"\n\n      conversation.end!\n    end\n\n    Rails.logger.info \"‚úÖ ConversationTimeoutMonitorJob completed (ended #{conversations_to_end.count} conversations)\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå ConversationTimeoutMonitorJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/conversation_memory_extraction_job.rb`:\n\n```rb\n# app/jobs/conversation_memory_extraction_job.rb\n\nclass ConversationMemoryExtractionJob < ApplicationJob\n  queue_as :default\n\n  BATCH_SIZE = 10 # Process up to 10 conversations at once\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üß† ConversationMemoryExtractionJob starting\"\n\n    conversations_without_memories = find_conversations_needing_memories\n\n    if conversations_without_memories.empty?\n      Rails.logger.info \"‚úÖ No conversations need memory extraction\"\n    else\n      Rails.logger.info \"üîç Found #{conversations_without_memories.count} conversations needing memory extraction\"\n\n      conversations_without_memories.in_batches(of: BATCH_SIZE) do |batch|\n        extract_memories_for_batch(batch)\n      end\n    end\n\n    Rails.logger.info \"‚úÖ ConversationMemoryExtractionJob completed\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå ConversationMemoryExtractionJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\n\n  private\n\n  def find_conversations_needing_memories\n    # Find finished conversations that don't have any memories yet\n    Conversation.finished\n      .left_joins(:conversation_memories)\n      .where(conversation_memories: { id: nil })\n      .includes(:conversation_logs)\n  end\n\n  def extract_memories_for_batch(conversations)\n    Rails.logger.info \"üîÑ Processing batch of #{conversations.count} conversations\"\n\n    conversation_data = prepare_conversation_data(conversations)\n    return if conversation_data.empty?\n\n    memories = extract_memories_with_llm(conversation_data)\n    store_extracted_memories(memories)\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå Batch memory extraction failed: #{e.message}\"\n  end\n\n  def prepare_conversation_data(conversations)\n    conversations.map do |conversation|\n      logs = conversation.conversation_logs.chronological\n      next if logs.empty?\n\n      {\n        session_id: conversation.session_id,\n        persona: conversation.persona,\n        started_at: conversation.started_at,\n        ended_at: conversation.ended_at,\n        duration: conversation.duration,\n        logs: logs.map do |log|\n          {\n            user_message: log.user_message,\n            ai_response: log.ai_response,\n            created_at: log.created_at\n          }\n        end\n      }\n    end.compact\n  end\n\n  def extract_memories_with_llm(conversation_data)\n    prompt = build_memory_extraction_prompt(conversation_data)\n\n    response = LlmService.generate_text(\n      prompt: prompt,\n      system_prompt: build_system_prompt,\n      model: \"google/gemini-2.5-flash\",\n      temperature: 0.3,\n      max_tokens: 2000\n    )\n\n    parse_memory_response(response)\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå LLM memory extraction failed: #{e.message}\"\n    []\n  end\n\n  def build_system_prompt\n    <<~PROMPT\n      You are a memory extraction system. Analyze multiple conversations and extract pertinent memories.\n\n      For each conversation, identify:\n      1. **preferences** - User likes, dislikes, choices (importance 1-10)\n      2. **facts** - Concrete information about the user's life/world (importance 1-10)#{'  '}\n      3. **instructions** - Things the user wants done or remembered (importance 1-10)\n      4. **context** - Important situational details (importance 1-10)\n      5. **events** - Significant happenings or outcomes (importance 1-10)\n\n      Return JSON array with format:\n      [\n        {\n          \"session_id\": \"session_123\",\n          \"memory_type\": \"preference\",\n          \"summary\": \"User prefers morning coffee over tea\",\n          \"importance\": 5,\n          \"metadata\": {\"extracted_from\": \"conversation_logs\"}\n        }\n      ]\n\n      Only extract memories that are actually useful for future interactions. Skip small talk.\n    PROMPT\n  end\n\n  def build_memory_extraction_prompt(conversation_data)\n    <<~PROMPT\n      Extract pertinent memories from these #{conversation_data.length} finished conversations:\n\n      #{format_conversations_for_prompt(conversation_data)}\n\n      Return only valuable memories that would be useful for future interactions.\n    PROMPT\n  end\n\n  def format_conversations_for_prompt(conversation_data)\n    conversation_data.map do |conv|\n      <<~CONV\n        === Conversation #{conv[:session_id]} ===\n        Persona: #{conv[:persona]}\n        Duration: #{conv[:duration]&.round(2)} seconds\n        Started: #{conv[:started_at]}\n\n        Exchanges:\n        #{format_conversation_logs(conv[:logs])}\n\n      CONV\n    end.join(\"\\n\")\n  end\n\n  def format_conversation_logs(logs)\n    logs.map do |log|\n      \"User: #{log[:user_message]}\\nAI: #{log[:ai_response]}\\n\"\n    end.join(\"\\n\")\n  end\n\n  def parse_memory_response(response)\n    JSON.parse(response)\n  rescue JSON::ParserError => e\n    Rails.logger.error \"‚ùå Failed to parse memory JSON: #{e.message}\"\n    Rails.logger.error \"Response was: #{response}\"\n    []\n  end\n\n  def store_extracted_memories(memories)\n    memories.each do |memory_data|\n      begin\n        ConversationMemory.create!(\n          session_id: memory_data[\"session_id\"],\n          memory_type: memory_data[\"memory_type\"],\n          summary: memory_data[\"summary\"],\n          importance: memory_data[\"importance\"],\n          metadata: memory_data[\"metadata\"]&.to_json\n        )\n\n        Rails.logger.info \"üíæ Stored #{memory_data['memory_type']} memory for #{memory_data['session_id']}\"\n      rescue StandardError => e\n        Rails.logger.error \"‚ùå Failed to store memory: #{e.message}\"\n        Rails.logger.error \"Memory data: #{memory_data.inspect}\"\n      end\n    end\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/goal_monitor_job.rb`:\n\n```rb\n# app/jobs/goal_monitor_job.rb\n\nclass GoalMonitorJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    return unless Rails.env.production? || Rails.env.development?\n\n    Rails.logger.info \"üéØ GoalMonitorJob starting\"\n\n    # Check if we need to switch to safety goals\n    check_safety_conditions\n\n    # Check if current goal has expired\n    check_goal_expiration\n\n    # Auto-complete goals if the persona has indicated completion\n    check_for_goal_completion\n\n    Rails.logger.info \"‚úÖ GoalMonitorJob completed successfully\"\n  rescue StandardError => e\n    Rails.logger.error \"‚ùå GoalMonitorJob failed: #{e.message}\"\n    Rails.logger.error e.backtrace.join(\"\\n\")\n  end\n\n  private\n\n  def check_safety_conditions\n    safety_active = GoalService.safety_mode_active?\n    current_goal = GoalService.current_goal_status\n\n    return unless current_goal\n\n    # If safety mode is active but we don't have a safety goal, switch\n    if safety_active && !current_goal[:category].include?(\"safety\")\n      Rails.logger.info \"‚ö†Ô∏è Safety mode active - switching to safety goal\"\n      GoalService.request_new_goal(reason: \"safety_mode_activated\")\n    # If safety mode is off but we have a safety goal, switch to regular goal\n    elsif !safety_active && current_goal[:category].include?(\"safety\")\n      Rails.logger.info \"‚úÖ Safety mode deactivated - switching to regular goal\"\n      GoalService.request_new_goal(reason: \"safety_mode_deactivated\")\n    end\n  end\n\n  def check_goal_expiration\n    return unless GoalService.goal_expired?\n\n    Rails.logger.info \"‚è∞ Goal expired - completing and selecting new goal\"\n    GoalService.complete_goal(completion_notes: \"Goal expired after time limit\")\n    GoalService.select_goal # Select new goal with default time limit\n  end\n\n  def check_for_goal_completion\n    # Look for recent conversation logs that indicate goal completion\n    recent_logs = ConversationLog.where(\"created_at > ?\", 10.minutes.ago)\n                                .order(created_at: :desc)\n                                .limit(10)\n\n    return if recent_logs.empty?\n\n    goal_completion_phrases = [\n      /goal\\s+(complete|completed|done|finished)/i,\n      /i\\s+(completed|finished|achieved)\\s+my\\s+goal/i,\n      /mission\\s+(accomplished|complete)/i,\n      /task\\s+(complete|completed|done)/i,\n      /i.*did\\s+it/i,\n      /success.*goal/i\n    ]\n\n    recent_logs.each do |log|\n      response = log.ai_response.to_s.downcase\n\n      if goal_completion_phrases.any? { |phrase| response.match?(phrase) }\n        Rails.logger.info \"üéâ Detected goal completion in conversation - completing goal\"\n        GoalService.complete_goal(completion_notes: \"Persona indicated goal completion\")\n        GoalService.select_goal # Select new goal\n        break # Only process one completion per run\n      end\n    end\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/application_job.rb`:\n\n```rb\nclass ApplicationJob < ActiveJob::Base\n  # Automatically retry jobs that encountered a deadlock\n  # retry_on ActiveRecord::Deadlocked\n\n  # Most jobs are safe to ignore if the underlying records are no longer available\n  # discard_on ActiveJob::DeserializationError\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/host_registration_job.rb`:\n\n```rb\n# frozen_string_literal: true\n\n# Job to register this server's IP with Home Assistant\n# Runs every 5 minutes to ensure registration stays current\nclass HostRegistrationJob < ApplicationJob\n  queue_as :default\n\n  def perform\n    Rails.logger.info \"üè† Running host registration job\"\n\n    begin\n      # Get the current server's IP and port\n      port = ENV.fetch(\"PORT\", 4567)\n\n      # Try to get the external IP (preferably Tailscale IP)\n      host_ip = ENV[\"SERVER_HOST\"] || get_external_ip\n\n      if host_ip\n        Rails.logger.info \"üîÑ Updating Home Assistant host registration: #{host_ip}:#{port}\"\n\n        # Update the input_text.glitchcube_host entity\n        hass_service = HomeAssistantService.new\n\n        result = hass_service.call_service(\n          \"input_text\",\n          \"set_value\",\n          entity_id: \"input_text.glitchcube_host\",\n          value: host_ip\n        )\n\n        Rails.logger.info \"‚úÖ Host registration job completed successfully\"\n        Rails.logger.debug \"üì§ Service call result: #{result.inspect}\"\n\n      else\n        Rails.logger.warn \"‚ö†Ô∏è Could not determine host IP for registration\"\n      end\n\n    rescue HomeAssistantService::ConnectionError => e\n      Rails.logger.warn \"‚ö†Ô∏è Home Assistant not available for host registration: #{e.message}\"\n      Rails.logger.warn \"üí° This is normal if Home Assistant is not running\"\n      # Don't re-raise - this is expected when HA is unavailable\n    rescue StandardError => e\n      Rails.logger.error \"‚ùå Host registration job failed: #{e.message}\"\n      Rails.logger.error e.backtrace.join(\"\\n\")\n      raise e # Re-raise to trigger SolidQueue retry logic\n    end\n  end\n\n  private\n\n  def get_external_ip\n    # Try multiple methods to get the external IP\n\n    # Method 1: Check for Docker/container environment variables\n    return ENV[\"HOST_IP\"] if ENV[\"HOST_IP\"]\n\n    # Method 2: Try to get IP from network interfaces (works in most environments)\n    require \"socket\"\n    begin\n      # Connect to a remote address to determine which interface to use\n      udp_socket = UDPSocket.new\n      udp_socket.connect(\"8.8.8.8\", 80)\n      ip = udp_socket.addr.last\n      udp_socket.close\n      return ip if ip && ip != \"127.0.0.1\"\n    rescue StandardError => e\n      Rails.logger.debug \"Failed to get IP via socket method: #{e.message}\"\n    end\n\n    # Method 3: Parse network interfaces directly\n    begin\n      Socket.ip_address_list.each do |addr|\n        next unless addr.ipv4?\n        next if addr.ipv4_loopback?\n        next if addr.ipv4_multicast?\n        # Prefer non-private IPs, but accept private ones as fallback\n        return addr.ip_address\n      end\n    rescue StandardError => e\n      Rails.logger.debug \"Failed to get IP via interface parsing: #{e.message}\"\n    end\n\n    # Method 4: Fallback to external service (use sparingly)\n    begin\n      require \"net/http\"\n      response = Net::HTTP.get_response(URI(\"http://checkip.amazonaws.com/\"))\n      return response.body.strip if response.code == \"200\"\n    rescue StandardError => e\n      Rails.logger.debug \"Failed to get IP via external service: #{e.message}\"\n    end\n\n    nil\n  end\nend\n\n```\n\n`/Users/estiens/code/glitchcube-main/glitchcube_rails/app/jobs/world_state_updaters/narrative_conversation_sync_job.rb`:\n\n```rb\n# app/jobs/world_state_updaters/narrative_conversation_sync_job.rb\n\nclass WorldStateUpdaters::NarrativeConversationSyncJob < ApplicationJob\n  queue_as :default\n\n  retry_on StandardError, wait: :exponentially_longer, attempts: 3\n\n  def perform(conversation_log_id)\n    conversation_log = ConversationLog.find(conversation_log_id)\n    WorldStateUpdaters::NarrativeConversationSyncService.sync_conversation(conversation_log)\n  rescue ActiveRecord::RecordNotFound => e\n    Rails.logger.error \"ConversationLog #{conversation_log_id} not found for HA sync: #{e.message}\"\n  rescue StandardError => e\n    Rails.logger.error \"Failed to sync conversation #{conversation_log_id} to HA: #{e.message}\"\n    raise e # Re-raise to trigger retry\n  end\nend\n\n```",
  "token_count": 0
}
